---
title: 爬虫教程
tags: bhubub
date: 2024-08-07 17:55:03
---

# 爬虫教程

## 第一课

```python
# -*- codeing =utf-8 -*-
# @Time :2024/7/21 10:57
# @Auther: Christinmin
# @file : .py
# @software:PyCharm
from urllib.request import urlopen
url="http://www.baidu.com"
resp=urlopen(url)
with open("mybaidu.html",mode="w",encoding='utf-8')as f:
    f.write(resp.read().decode("UTF-8"))   #读取网页的页面源代码 到mybaidu.html中
print("over!")
```

## 第二课

1. 服务器渲染：在服务器直接将数据和 html 整合在一起，统一发送给服务器
2. 客户端渲染：第一次结果只有一个 html 骨架，第二次请求结果才出现数据，并进行数据展示，因此在页面源代码中看不到数据
3.

## 第三课

http 协议

**请求**

请求行：请求方式（get/post)请求 url 协议 地址

请求头：放一些服务器使用的附加信息（需要啥，need）

请求体：放请求参数，比如说 jpg，url

**响应**

状态行：协议，状态码

响应头：放一些客户端需要的信息

响应体：需要恢复的内容

1. 请求方式：

   get 直接获取

   post 修改

2.

## 第三节课

**request**

1. get 方式

```python
import  requests
url="https://www.google.com"
dic={

	"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:127.0) Gecko/20100101 Firefox/127.0"
}
resp=requests.get(url,headers=dic)
print(resp)
print(resp.url)
```

2. post 方式请求

```python
import requests
url="https://fanyi.baidu.com/sug"
s=input("输入你要翻译的")
dat={
    "kw":s
}
//data必须要求，需要输入input然后作为字典传递到data中
resp=requests.post(url,data=dat)
print(resp.json())
```

**抓取某个以客户端为主的排行榜**

```python
import requests

url="https://movie.douban.com/j/chart/top_list?"
param={
"type": "24",
"interval_id": "100:90",
"action": "",
"start": 0,
"limit": 20,
}
headers={
"User-Agent":
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0"
}
resp=requests.get(url=url,params=param,headers=headers)
print(resp.json())
print(resp.url)
resp.close()
```

## 第三课

数据解析概述

1. re 解析
2. bs4 解析
3. xpath 解析

---

### re 解析（正则表达式）regular expression

一种使用表达式方式对字符串进行匹配的语法规则

| 符号 |         代表意思         |                 使用场景                 |     |     |     |     |     |     |
| ---- | :----------------------: | :--------------------------------------: | --- | --- | --- | --- | --- | --- | --- | --- |
| \d   |         数字 0-9         |         \d 匹配单个数字，1、2、3         |     |     |     |     |     |     |
| ^    |      匹配字符串开始      |   比如 a12345322 用^\d\*8 不能匹配出来   |     |     |     |     |     |     |
| $    |      匹配字符串结束      |               用法同$一致                |     |     |     |     |     |     |     |     |
| \D   |          非数字          |        \D 匹配单个非数字，a、-、'        |     |     |     |     |     |     |
| \w   |    字母、数字、下划线    | \w 匹配单个字母、数字、下划线，a、1、\_  |     |     |     |     |     |     |
| \W   |   非字母、数字、下划线   | \W 匹配单个非字母、数字、下划线，{、-、[ |     |     |     |     |     |     |
| \s   |         空白字符         |   \s 匹配单个空格、回车(\n)、制表符(\    |     |     |     |     |     |     |
| \S   |        非空白字符        | \S 匹配单个非空格、换行(\n)、制表符(\t)  |     |     |     |     |     |     |
| .    | 匹配除换行之外的所有字符 |          . 单个换行之外所有字符          |     |     |     |     |     |     |
| \|   |           或者           |          a\|b 匹配字符 a 或者 b          |     |     |     |     |     |     |

量词：控制元字符出现的次数

| 符号  |    代表意思     |            使用场景            |
| ----- | :-------------: | :----------------------------: |
| \*    | 0 次及 0 次以上 |   \d\* 匹配数字，123、02、空   |
| +     | 1 次及 1 次以上 |    \d+ 匹配数字，123、02、3    |
| ?     |   0 次或 1 次   |      \d? 匹配数字，2、空       |
| {m,n} |   m 次到 n 次   |  \d{1,3} 匹配数字，2、12、123  |
| {n}   |      n 次       |       \d{2} 匹配数字，12       |
| {n,}  | n 次及 n 次以上 | \d{2,} 匹配数字，12、123、1234 |

```
.*？尽可能少的匹配  ->   <---
.* 贪婪匹配，尽可能多的匹配
```

例如：

.\*?x 在 huihiqhqxidiqwodiwdjx

则能匹配出

huihiqhx

qwodiwdjx

## 第五课

列举出豆瓣 top250 的排名

```python
# -*- codeing =utf-8 -*-
# @Time :2024/7/26 11:29
# @Auther: Christinmin
# @file : .py
# @software:PyCharm
import csv

import requests
import  re
url="https://movie.douban.com/top250"
headers={
"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0"
}
resp=requests.get(url,headers=headers)
# print(resp.text)
page_content=resp.text
obj=re.compile(r'<li>.*?<div class="item">.*?<span class="title">(?P<name>.*?)'
               r'</span>.*?  <p class="">.*?<br>(?P<year>.*?)&nbsp.*?'
               r'<span class="rating_num" property="v:average">(?P<score>.*?)</span>.*?'
               r'<span>(?P<comment>.*?)人评价</span>.*?'
               r' <span class="inq">(?P<theme>.*?)</span>',re.S)
result=obj.finditer(page_content)
# //作为字典在文件中输出
f=open("data.csv",mode="w")
csvwriter=csv.writer(f)

for it in result:
    print(it.group("name"))
    print(it.group("year").strip())
    print(it.group("score"))
    print(it.group("comment"))
    print(it.group("theme"))
    dic=it.groupdict()
    dic['year']=dic['year'].strip()
    csvwriter.writerow(dic.values())
    print("over")
```

# 第五课 获取子页面的名字和地址

```python
# -*- codeing =utf-8 -*-
# @Time :2024/7/26 14:33
# @Auther: Christinmin
# @file : .py
# @software:PyCharm
import  requests
import  re
domain="https://www.dyttcn.com/"
headers={
"User-Agent":
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0"
}
resp=requests.get(domain,headers=headers,verify=False)
resp.encoding='gb2312'
# print(resp.text)
#用正则表达式筛选出里面的区域以及地址
obj1=re.compile(r'最新更新.*?<ul>(?P<ul>.*?)</ul>',re.S)
obj2=re.compile(r"<a href='(?P<href>.*?)'",re.S)
obj3=re.compile(r'◎片　　名(?P<movie>.*?)</p>.*?'
                r'<iframe allowfullscreen="true" border="0" frameborder="0" framespacing="0" height="400" marginheight="0" marginwidth="0" mozallowfullscreen="true" noresize="noresize" scrolling="no" src="(?P<download>.*?)"',re.S)

result1=obj1.finditer(resp.text)
child_href_list=[]
for it in result1:
    # print(it.group())
    ul=it.group('ul')

    # 提取子页面链接
    result2=obj2.finditer(ul)
    for itt in result2:
         child_href=domain+itt.group('href').strip("/")
         child_href_list.append(child_href)
    获取子页面的内容和连接地址
for href in child_href_list:
     child_resp=requests.get(href,verify=False)
     child_resp.encoding='gb2312'
     # print(child_resp.text)
     result3=obj3.search(child_resp.text)
     print(result3.group('movie'))
     print(result3.group("download"))
     # break
     print()
```

# 爬取子链接中的图片

```python
# -*- codeing =utf-8 -*-
# @Time :2024/7/26 16:04
# @Auther: Christinmin
# @file : .py
# @software:PyCharm
import requests
from bs4 import  BeautifulSoup
import  time
domain="http://www.umeituku.com/bizhitupian/weimeibizhi/"

resp=requests.get(domain)
# print(resp.text)
resp.encoding = "utf-8"

# 解析数据
# 把数据源代码交给beautifulsoup进行处理，生成bs对象
page=BeautifulSoup(resp.text,"html.parser")  #指定html的解析器
alllist=table=page.find("div",class_="TypeList").find_all("a")
for a in alllist:
    # print(a.get('href'))
    href=a.get('href')
    if not href.startswith("http"):
        href = domain + href
    child_page_resp=requests.get(href,verify=False)
    child_page_resp.encoding="utf-8"
    child_page_text=child_page_resp.text
    # 从子页面中拿到图片的下载路径
    child_page=BeautifulSoup(child_page_text,"html.parser")
    imgs = child_page.find_all("img")
    for img in imgs:
        src = img.get("src")
        if src:
            print(src)
            # 下载图片
            img_resp = requests.get(src, verify=False)  # 禁用证书验证
            if "image" in img_resp.headers.get("Content-Type", ""):
              img_name = src.split("/")[-1]  # 拿到url最后一个链接
              if not img_name:
                  img_name = "default.png"  # 提供一个默认文件名
              with open(img_name, mode="wb") as f:
                  f.write(img_resp.content)
              print("Downloaded:", img_name)
              time.sleep(1)  # 防止请求过于频繁
    # p=child_page.find("p",align="center")
#     a=1
#     if p is not None:
#         img=p.find("img")
#         if img is not None:
#             src=img.get("src")
#
#               # print(img.get("src"))
# #     # 下载图片
#             img_resp=requests.get(src,verify=False)
#             img_name=src.split("/")[1]
#             if not img_name:
#                 img_name = "default.jpg"
#             with open(img_name,mode="wb")as f:
#                 f.write(img_resp.content)
#             print("over",img_name)
#             print(a)
#             a+=1
#             time.sleep(1)
```

# 利用 xpath 来爬虫（vital）

```python
# -*- codeing =utf-8 -*-
# @Time :2024/7/28 11:14
# @Auther: Christinmin
# @file : .py
# @software:PyCharm
import  requests
from lxml import  etree

url="https://www.zbj.com/fw/?k=网站定制开发"
headers={
"user-agent":
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36 Edg/127.0.0.0"


}
resp=requests.get(url,headers=headers)
# print(resp.text)

# 解析
html=etree.HTML(resp.text)
# 拿到每一个div的服务商的信息
divs=html.xpath('//*[@id="__layout"]/div/div[3]/div[1]/div[4]/div/div[2]/div/div[2]/div')
for it in divs:
    print(it.xpath("./div/div[3]/div[1]/span/text()")," ",it.xpath("./div / div[5] / div / div / div/text()"))
    # print(it.xpath("./div / div[5] / div / div / div/text()"))

if not divs:
    print("找不到任何元素")
else:
    print("找到的元素个数为:",len(divs))

```

# 第三章：request 的进阶概述

我们在之前的爬虫其实已经使用过 headers 了，headers 是为了 http 协议中的请求头。一般存放一些请求内容相关的无关数据，有时也会存放一些安全验证信息，比如常见的 user-agent、token、cookie

通过 request 发送的请求，我们可以把请求头的信息放在 headers 里，也可以单独进行存放，最终通过 request 自动帮我们拼接成完整的 http 请求头

内容：

1. 模拟浏览器登录-->处理 cookie
2. 防盗链处理--->抓取梨视频数据
3. 代理----->防止被封 ip

cookie 的作用：、

用户账号、密码登录后-------->服务端进行验证

用户的 cookie 存储 <----------服务端之前存档的信息给对方的 cookie

用户请求下载----------->服务端

    <-------------

Cookie 是在网络浏览中非常重要的技术，其主要作用包括以下几个方面：

1. **会话管理**：Cookies 可以存储用户的会话信息，例如登录状态、购物车内容、偏好设置等。这样，当用户在同一个网站上浏览不同页面或重新访问网站时，网站能够识别用户并恢复其会话。
2. **个性化设置**：Cookies 可以存储用户的个性化设置，例如语言选择、主题偏好等。通过这些信息，网站可以为用户提供更加个性化的体验。
3. **跟踪和分析**：Cookies 可以用于跟踪用户在网站上的行为，例如访问的页面、停留时间、点击的链接等。网站可以通过这些数据进行分析，了解用户行为和偏好，从而改进网站的内容和结构，提供更好的用户体验。
4. **广告投放**：Cookies 可以用于广告网络跟踪用户的浏览行为，以便为用户提供更有针对性的广告。这些广告通常基于用户的兴趣和浏览历史，以提高广告的相关性和效果。
5. **安全性**：Cookies 可以用于增强网站的安全性，例如防止跨站请求伪造（CSRF）攻击、检测用户的登录状态等。

Session（会话）在 Web 开发中扮演着重要的角色，主要用于在客户端和服务器之间保持状态信息。由于 HTTP 协议是无状态的，这意味着每个请求都是独立的，服务器不会记住之前的请求。为了在用户和服务器之间保持连续的交互，Session 应运而生。以下是 Session 的主要作用：

1. **用户身份验证**：
   - 当用户登录网站时，服务器会创建一个 Session，并为该 Session 分配一个唯一的 Session ID。这个 Session ID 通常会存储在 Cookie 中，并发送到客户端。
   - 在后续请求中，客户端会将这个 Session ID 发送回服务器，服务器通过这个 ID 识别用户的身份，从而保持用户的登录状态。
2. **存储用户数据**：
   - Session 可以用来存储用户的数据和偏好设置，例如购物车内容、浏览历史、表单数据等。这些数据可以在用户会话期间被多次访问和更新。
   - 这些数据存储在服务器端，相对于存储在客户端的 Cookie 来说，Session 更加安全，因为它不容易被用户篡改。
3. **状态管理**：
   - Session 在管理应用程序的状态方面非常有用。通过 Session，服务器可以跟踪用户的活动，例如当前所在的页面、填写的表单数据等。这对于复杂的 Web 应用程序特别重要。
   - 例如，在多步骤的表单提交过程中，Session 可以保存每一步的数据，确保用户能够继续未完成的操作而不会丢失数据。
4. **增强安全性**：
   - Session 提供了一种更安全的方式来管理用户的敏感信息，因为数据存储在服务器端，不容易被恶意用户获取和篡改。
   - Session ID 的生成和管理也需要考虑安全性，防止 Session 劫持和会话固定攻击。

![img](https://i.loli.net/2019/12/25/LjW2CfNSD7OaY4v.jpg)
